{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# parmeters\n",
    "T_in      = 10\n",
    "#T_pred    = 20 - T_in\n",
    "T_pred    = 1\n",
    "BATCH     = 64 \n",
    "#data     = tf.zeros( [T, BATCH, 32, 32, 128] )\n",
    "num_steps = 1000\n",
    "K         = 10 # number of frames used in future prediction\n",
    "std_dev   = 0.1\n",
    "\n",
    "\n",
    "def conv2d( x, W, b, strides = 1 ):\n",
    "  x = tf.nn.conv2d(x, W, strides = [ 1, strides, strides, 1], padding = 'SAME')\n",
    "  x = tf.nn.bias_add(x,b)\n",
    "  return tf.nn.relu(x)\n",
    "\n",
    "def fc( x, W, b):\n",
    "  x = tf.matmul(x, W) + b\n",
    "  return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "class LSTMAutoEncoder(object):\n",
    "  def __init__(self, **kwargs):\n",
    "\n",
    "    self.weights = {\n",
    "        'wc1':  tf.Variable( tf.random.normal( [5, 5,  1, 24], stddev= std_dev ) ),\n",
    "        'wc2':  tf.Variable( tf.random.normal( [5, 5, 24, 64], stddev= std_dev ) ),\n",
    "        'wc3':  tf.Variable( tf.random.normal( [5, 5, 64, 64], stddev= std_dev ) ),\n",
    "        'wfc1': tf.Variable( tf.random.normal( [1024, 4096],   stddev= std_dev) )  \n",
    "    }\n",
    "    \n",
    "    self.biases = {\n",
    "        'bc1' : tf.Variable( tf.random.normal([24],  stddev = 0) ),\n",
    "        'bc2' : tf.Variable( tf.random.normal([64],  stddev = 0) ),\n",
    "        'bc3' : tf.Variable( tf.random.normal([64],  stddev = 0) ), \n",
    "        'bfc1': tf.Variable( tf.random.normal([4096],stddev = 0) )\n",
    "    }\n",
    "    \n",
    "  \n",
    "  def EncoderDecoder(self, data ):\n",
    "    \n",
    "    # Encoder Decoder works over 2*T timesteps\n",
    "    # First phase: Encoding\n",
    "    # Put in real input data, discard output, keep state\n",
    "    # Second phase: Decoding\n",
    "    # Put in zero data (padding), use output/state\n",
    "\n",
    "    with tf.variable_scope( \"LSTM\" ) as scope:\n",
    "    \n",
    "      lstm  = tf.contrib.rnn.LSTMCell( num_units = 1024 ) \n",
    "      state = lstm.zero_state( BATCH, \"float\" ) \n",
    "      datum = tf.split(data, T_in, axis = 1) # datum order: batch_size, T, height*width*channel\n",
    "\n",
    "      # run lstm for T time step\n",
    "      for t in range(T_in):\n",
    "        if t > 0: \n",
    "            scope.reuse_variables()  # vary important! \n",
    "        \n",
    "        output, state = lstm( tf.reshape( datum[t], [BATCH, -1] ), state) # inputs: 2-D tensor with shape [batch_size x input_size], state\n",
    "\n",
    "      # what is tmp? datum at frame 0, why need to reshape it?\n",
    "      tmp   = tf.reshape( datum[0], [BATCH, -1] )\n",
    "\n",
    "      # Decoding phase\n",
    "      zero_ = tf.zeros_like( tmp, \"float\" ) # generate a zero array using the shape of tmp\n",
    "\n",
    "      output_list = []\n",
    "      for t in range(T_pred):\n",
    "        scope.reuse_variables() # this is important!      \n",
    "\n",
    "        output, state = lstm( zero_, state ) # what is state here? \n",
    "        #output_list.append( tf.reshape( output, [BATCH,1,-1] ) ) # modify! \n",
    "        output_list.append( output )\n",
    "      \n",
    "      #print \"output_list len\", output_list.__len__()\n",
    "      out = tf.concat( output_list, axis = 1 ) # ?\n",
    "      return tf.reshape( out, [BATCH*T_pred, -1] )\n",
    "\n",
    "  def load_validation(self):\n",
    "    file = np.load( '/home/fensi/nas/Moving-MNIST/moving-mnist-valid.npz' )\n",
    "    print(file.keys())\n",
    "    # ['clips', 'dims', 'input_raw_data']\n",
    "    data = file['input_raw_data']\n",
    "    return data\n",
    "\n",
    "  def load_training(self):\n",
    "    data = np.load( 'mnist_test_seq.npy' )\n",
    "    # ['clips', 'dims', 'input_raw_data']\n",
    "    #data = file['input_raw_data'] # (200K, 1, 64, 64) --> (10K, 20, 64, 64)-> number of sequences, frames/sequence, height, width \n",
    "    data = np.reshape( data, [-1, 20, 64, 64, 1] )\n",
    "     \n",
    "    print(\"loading training data: data.shape\", data.shape)\n",
    "    \n",
    "    input_seq = data[:, 0:10]\n",
    "    output_seq = data[:, 10:]\n",
    "    \n",
    "    return input_seq, output_seq\n",
    "\n",
    "  def load_testing(self):\n",
    "    data = np.load( 'mnist_test_seq.npy' )\n",
    "    # ['clips', 'dims', 'input_raw_data']\n",
    "    data = np.reshape ( data, [-1, 20, 64, 64, 1] )\n",
    "    input_seq = data [ :, 0:10]\n",
    "    output_seq = data [ :, 10:] \n",
    "    return input_seq, output_seq\n",
    "\n",
    "# Data\n",
    "net             = LSTMAutoEncoder()\n",
    "inp,out         = net.load_training()\n",
    "tst_in, tst_out = net.load_testing() \n",
    "\n",
    "# Shuffle the sequence\n",
    "perm            = list(range( inp.shape[0] )) # inp.shape[0] : 10k\n",
    "random.shuffle( perm )\n",
    "\n",
    "\n",
    "# Construct model\n",
    "X               = tf.placeholder( \"float\", [BATCH, T_in,   64, 64, 1] ) # in TensorFlow, channel has to be at the last place\n",
    "Y               = tf.placeholder( \"float\", [BATCH, T_pred, 64, 64, 1] ) \n",
    "\n",
    "# Conv2D with stride 2 (3 times)\n",
    "# Reshape tensor to 4D (Conv2d only supports 4D tensors not 5D)\n",
    "X_shape         = tf.reshape( X, [BATCH*T_in, 64, 64, 1] )\n",
    "Y_shape         = tf.reshape( Y, [BATCH, T_pred, -1] )  # for computing loss, reshape as 3-D tensor\n",
    "\n",
    "conv1           = conv2d( X_shape,   net.weights['wc1'], net.biases['bc1'], 2 )\n",
    "conv2           = conv2d( conv1,     net.weights['wc2'], net.biases['bc2'], 2 )\n",
    "conv3           = conv2d( conv2,     net.weights['wc3'], net.biases['bc3'], 2 )\n",
    "\n",
    "# Reshape tensor to [BATCH, T, DIM]\n",
    "res             = tf.reshape( conv3, [BATCH, T_in, -1] )\n",
    "\n",
    "# LSTM encoders/decoders\n",
    "# Input T_in frames, Output T_pred frames\n",
    "prediction      = net.EncoderDecoder(res)\n",
    "#exit(0)\n",
    "print(\"prediction\", prediction.get_shape().as_list()) # prediction [T * BATCH, 1024] = 10x BATCH (number of sequence you process in parallel, the output is one feature for each frame )\n",
    "\n",
    "# Fully connected: (BATCH*10, 1024) -->(BATCH*10, 4096) \n",
    "fc_out          = fc( prediction, net.weights['wfc1'], net.biases['bfc1'] )\n",
    "print(\"fc_out shape\", fc_out.shape)\n",
    "\n",
    "\n",
    "# Reshape fc_out to BATCH x T x DIM\n",
    "fc_out          = tf.reshape( fc_out, [BATCH, T_pred, -1] )\n",
    "sig_out         = tf.sigmoid( fc_out )\n",
    "\n",
    "\n",
    "# define loss and optimizer\n",
    "#loss_op         = tf.reduce_sum( tf.nn.sigmoid_cross_entropy_with_logits( logits = fc_out, labels = Y_shape ), axis = 1 ) # batch x 10 --> batch x 1\n",
    "#loss_op         = tf.reduce_sum( loss_op, axis=1 )\n",
    "#loss_op         = tf.reduce_mean( loss_op, axis=0 ) # loss is averaged over the batch  \n",
    "\n",
    "diff            = fc_out - Y_shape\n",
    "loss_op         = 0.5 * tf.reduce_sum( tf.reduce_sum( diff * diff, axis = 2 ), axis = 1)\n",
    "loss_op         = tf.reduce_mean( loss_op )\n",
    "train_op        = tf.train.AdamOptimizer( learning_rate = 0.001 ).minimize(loss_op)\n",
    "\n",
    "\n",
    "print(\"inp.shape\", inp.shape)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  init            = tf.global_variables_initializer()\n",
    "  sess.run( init )\n",
    "  \n",
    "  # Number of Epochs\n",
    "  for e in range( 50 ):\n",
    "    # Iterations\n",
    "    for start in range( 0, inp.shape[0]-(inp.shape[0]%BATCH), BATCH  ):\n",
    "      batch_x = np.zeros( [BATCH, T_in,   64, 64, 1] )\n",
    "      batch_y = np.zeros( [BATCH, T_pred, 64, 64, 1] )\n",
    "\n",
    "      for b in range( BATCH ):\n",
    "        batch_x[b] = inp[ perm[ start+b ], : T_in ]\n",
    "        batch_y[b] = out[ perm[ start+b ], : T_pred ]\n",
    "\n",
    "      feed = { X: batch_x, Y: batch_y }\n",
    "      op, loss, p1 = sess.run([train_op, loss_op, prediction], feed_dict = feed )\n",
    "      \n",
    "      print(loss)\n",
    "  \n",
    "  # Testing the reconstruction \n",
    "  batch_x      = inp[ 0 : BATCH ]\n",
    "  img_pre, img = sess.run( [fc_out, sig_out], feed_dict = { X : batch_x } )\n",
    "  \n",
    "  img_pre      = np.reshape( img_pre, [BATCH, T_pred, 64, 64] )\n",
    "  img          = np.reshape( img, [BATCH, T_pred, 64, 64] )\n",
    "  \n",
    "  for t in range( T_pred ):\n",
    "    plt.imshow( img_pre[0,t] )\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
